{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vaibh\\AppData\\Roaming\\Python\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_json(\"hf://datasets/toughdata/quora-question-answer-dataset/Quora-QuAD.jsonl\", lines=True) #pd.read_csv('qa_data.csv') #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Why whenever I get in the shower my girlfriend...</td>\n",
       "      <td>Isn’t it awful? You would swear that there was...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is a proxy, and how can I use one?</td>\n",
       "      <td>A proxy server is a system or router that prov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What song has the lyrics \"someone left the cak...</td>\n",
       "      <td>MacArthur's Park\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I am the owner of an adult website called http...</td>\n",
       "      <td>Don't let apps that are liers put adds on your...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Does the Bible mention anything about a place ...</td>\n",
       "      <td>St. John in the book of Revelation mentions an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  Why whenever I get in the shower my girlfriend...   \n",
       "1            What is a proxy, and how can I use one?   \n",
       "2  What song has the lyrics \"someone left the cak...   \n",
       "3  I am the owner of an adult website called http...   \n",
       "4  Does the Bible mention anything about a place ...   \n",
       "\n",
       "                                              answer  \n",
       "0  Isn’t it awful? You would swear that there was...  \n",
       "1  A proxy server is a system or router that prov...  \n",
       "2                                 MacArthur's Park\\n  \n",
       "3  Don't let apps that are liers put adds on your...  \n",
       "4  St. John in the book of Revelation mentions an...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing\n",
    "\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# init lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "# Regular expression pattern for matching URLs\n",
    "url_pattern = r'http\\S+|www\\S+|https\\S+'\n",
    "\n",
    "def clean_text(text):\n",
    "    \n",
    "    # Lemmatization\n",
    "    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "    \n",
    "    # Remove emojis, Hindi characters\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
    "    \n",
    "    # Remove URLs from text\n",
    "    text = re.sub(url_pattern, '', text, flags=re.MULTILINE)\n",
    "\n",
    "    return text\n",
    "\n",
    "df['question'] = df['question'].apply(clean_text)\n",
    "df['answer'] = df['answer'].apply(clean_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "class QADataset(Dataset):\n",
    "    def __init__(self, questions, answers, tokenizer, max_length):\n",
    "        self.questions = questions\n",
    "        self.answers = answers\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.questions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        question = self.questions[idx]\n",
    "        answer = self.answers[idx]\n",
    "\n",
    "        # Tokenize inputs and labels\n",
    "        input_encoding = self.tokenizer(\n",
    "            question,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        target_encoding = self.tokenizer(\n",
    "            answer,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        item = {\n",
    "            'input_ids': torch.squeeze(input_encoding['input_ids']),\n",
    "            'attention_mask': torch.squeeze(input_encoding['attention_mask']),\n",
    "            'labels': torch.squeeze(target_encoding['input_ids'])\n",
    "        }\n",
    "\n",
    "        return item\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "# Split dataset into training and validation sets\n",
    "train_df, val_df = train_test_split(df, test_size=0.2)\n",
    "\n",
    "# Initialize tokenizer and model\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small', use_fast=True)\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "max_length = 128\n",
    "train_dataset = QADataset(\n",
    "    train_df['question'].tolist(),\n",
    "    train_df['answer'].tolist(),\n",
    "    tokenizer,\n",
    "    max_length\n",
    ")\n",
    "val_dataset = QADataset(\n",
    "    val_df['question'].tolist(),\n",
    "    val_df['answer'].tolist(),\n",
    "    tokenizer,\n",
    "    max_length\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16)\n",
    "\n",
    "# Set device\n",
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n"
     ]
    }
   ],
   "source": [
    "print(\"CUDA Available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4 completed\n",
      "Validation Loss after Epoch 1: 2.489588534865771\n",
      "Epoch 2/4 completed\n",
      "Validation Loss after Epoch 2: 2.4361423161144633\n",
      "Epoch 3/4 completed\n",
      "Validation Loss after Epoch 3: 2.4125878017279647\n",
      "Epoch 4/4 completed\n",
      "Validation Loss after Epoch 4: 2.396877258425056\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 4\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        inputs = {\n",
    "            'input_ids': batch['input_ids'].to(device),\n",
    "            'attention_mask': batch['attention_mask'].to(device),\n",
    "            'labels': batch['labels'].to(device)\n",
    "        }\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs} completed\")\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            inputs = {\n",
    "                'input_ids': batch['input_ids'].to(device),\n",
    "                'attention_mask': batch['attention_mask'].to(device),\n",
    "                'labels': batch['labels'].to(device)\n",
    "            }\n",
    "            \n",
    "            outputs = model(**inputs)\n",
    "            loss = outputs.loss\n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    print(f\"Validation Loss after Epoch {epoch + 1}: {val_loss / len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('fine-tuned-t5\\\\tokenizer_config.json',\n",
       " 'fine-tuned-t5\\\\special_tokens_map.json',\n",
       " 'fine-tuned-t5\\\\spiece.model',\n",
       " 'fine-tuned-t5\\\\added_tokens.json')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"fine-tuned-t5\")\n",
    "tokenizer.save_pretrained(\"fine-tuned-t5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: lead generation and marketing are essential for various industries, a it involves a wide range of b2b (business-to-business) services, a it involves a combination of a wide range of b2b (business-to-business) services, a it involves a combination of a wide range of b2b (business-to-business) services, a it involves a combination of a wide range of b\n"
     ]
    }
   ],
   "source": [
    "question = \"what industry need lead generation and marketing\"\n",
    "\n",
    "# Encode the input\n",
    "input_ids = tokenizer.encode(question, return_tensors='pt').to(device)\n",
    "# Generate the answer\n",
    "outputs = model.generate(input_ids, max_length=100, early_stopping=True)\n",
    "\n",
    "# Decode the generated answer\n",
    "answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"Answer:\", answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
